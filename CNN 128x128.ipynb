{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!activate tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ./data/train-jpg/*.jpg | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv('./data/train-csv/train.csv')\n",
    "df_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_labels.tags.values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = []\n",
    "for lbl in labels:\n",
    "    labels_list.extend(lbl.split(' '))\n",
    "labels_set = set(labels_list)\n",
    "labels_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(labels_set):\n",
    "    print('label:%s  %d/%d' %(label, i+1, len(labels_set)))\n",
    "    %time df_labels[label] = df_labels.tags.apply(lambda x : 1 if label in x.split(' ') else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_labels.tags.str.get_dummies(' ')\n",
    "df.insert(0, 'image_name', df_labels.image_name)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[list(labels_set)].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[list(labels_set)].sum().sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_labels = df[list(labels_set)].sum().sort_values(ascending=False).index\n",
    "type(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ordered_labels].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occurence = df[ordered_labels].T.dot(df[ordered_labels])\n",
    "df_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_percentage = df[ordered_labels].sum() / df[ordered_labels].count() * 100\n",
    "df_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_percentage.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data/train-jpg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_graph(label):\n",
    "\n",
    "    images = df[df[label] == 1].image_name.values\n",
    "\n",
    "    fig , ax = plt.subplots(nrows=3, ncols=3, figsize=(8,8))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i in range(0,9):\n",
    "        f = random.choice(images)\n",
    "        img = Image.open(os.path.join(TRAIN_PATH, f + '.jpg'))\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_title(\"%s h:%s w:%s\" % (f, img.height,img.width))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_graph('primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_graph('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_graph('agriculture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_graph('habitation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test, df_y_train, df_y_test = train_test_split(df.image_name, df[ordered_labels], test_size=60, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, df_y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.set_index(np.arange(df_train.shape[0]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.reindex(index=np.arange(df_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test, df_y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.set_index(np.arange(df_test.shape[0]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_train['image_name_w_ext'] = df_train['image_name'] + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_test['image_name_w_ext'] =  df_test['image_name'] + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if not 'image_name_w_ext' in df_train.columns:\n",
    "#    %time df_train = df_train[['image_name', 'image_name_w_ext'] + list(df_train.columns.values[1:-1])]\n",
    "#    %time df_test = df_test[['image_name', 'image_name_w_ext'] + list(df_test.columns.values[1:-1])]\n",
    "#df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for f in df_train.image_name_w_ext:\n",
    "#    if os.path.exists(os.path.join(TRAIN_PATH, f)) == False:\n",
    "#        print(\"%s is missing\" % image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time shapes = [Image.open(os.path.join(TRAIN_PATH, f)).size for f in df_train.image_name_w_ext[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pd.Series(shapes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 9\n",
    "NB_MINI_BATCH = df_train.shape[0] / MINI_BATCH_SIZE\n",
    "SHUFFLE = False\n",
    "BATCH_SIZE = df_train.shape[0]\n",
    "#NB_CATEGORIES = len(ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time idx = np.random.permutation(len(df_train)) if SHUFFLE == True else np.arange(BATCH_SIZE)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time batches = np.array_split(idx, NB_MINI_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config.log_device_placement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32,shape=(None, 128,128,3), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.float32,shape=(None,17), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('CONV1'):\n",
    "    W_conv1 = tf.get_variable(name=\"W_conv1\", shape=[5,5,3,64], dtype=tf.float32, initializer= tf.contrib.layers.xavier_initializer())\n",
    "    b_conv1 = tf.get_variable(name=\"b_conv1\", shape=[64], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    conv1 = tf.nn.conv2d(\n",
    "        input=X,\n",
    "        filter=W_conv1,\n",
    "        strides=[1,1,1,1],\n",
    "        padding='SAME',\n",
    "        name='conv1'\n",
    "    ) #64 * 64 * 32\n",
    "\n",
    "    relu1 = tf.nn.elu(\n",
    "        features = conv1 + b_conv1,\n",
    "        name='relu1'\n",
    "    ) #64 * 64 * 32\n",
    "\n",
    "    pool1 = tf.nn.max_pool(\n",
    "        value = relu1,\n",
    "        ksize=[1,2,2,1],\n",
    "        strides=[1,2,2,1],\n",
    "        padding='SAME',\n",
    "        name='pool1'\n",
    "    ) #32 * 32 * 32\n",
    "    \n",
    "    tf.summary.histogram('W_conv1', W_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('CONV2'):\n",
    "    W_conv2 = tf.get_variable(name=\"W_conv2\", shape=[5,5,64,64], dtype=tf.float32, initializer= tf.contrib.layers.xavier_initializer())\n",
    "    b_conv2 = tf.get_variable(name=\"b_conv2\", shape=[64], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    conv2 = tf.nn.conv2d(\n",
    "        input=pool1,\n",
    "        filter=W_conv2,\n",
    "        strides=[1,1,1,1],\n",
    "        padding='SAME',\n",
    "        name='conv2'\n",
    "    ) #32 * 32 * 64\n",
    "\n",
    "    relu2 = tf.nn.elu(\n",
    "        features = conv2 + b_conv2,\n",
    "        name='relu2'\n",
    "    ) #32 * 32 *64\n",
    "\n",
    "    pool2 = tf.nn.max_pool(\n",
    "        value = relu2,\n",
    "        ksize=[1,2,2,1],\n",
    "        strides=[1,2,2,1],\n",
    "        padding='SAME',\n",
    "        name='pool2'\n",
    "    ) #16 * 16 *64\n",
    "    \n",
    "            \n",
    "    tf.summary.histogram('W_conv2', W_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('FC1'):\n",
    "    W_fc1 = tf.get_variable(name=\"W_fc1\", shape=[32*32*64,256], dtype=tf.float32, initializer= tf.contrib.layers.xavier_initializer())\n",
    "    b_fc1 = tf.get_variable(name=\"b_fc1\", shape=[256], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    pool_2_flat = tf.reshape(\n",
    "        tensor=pool2,\n",
    "        shape=(-1,32*32*64),\n",
    "        name='pool_2-flat'\n",
    "    )\n",
    "\n",
    "    fc1 = tf.nn.elu(tf.matmul(pool_2_flat,W_fc1) + b_fc1)\n",
    "    \n",
    "    tf.summary.histogram('W_fc1', W_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('DROPOUT'):\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    dropout = tf.nn.dropout(\n",
    "        x=fc1,\n",
    "        keep_prob=keep_prob\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('FC2'):\n",
    "    W_fc2 = tf.get_variable(name=\"W_fc2\", shape=[256, 17], dtype=tf.float32, initializer= tf.contrib.layers.xavier_initializer())\n",
    "    b_fc2 = tf.get_variable(name=\"b_fc2\", shape=[17], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    fc2 = tf.matmul(dropout, W_fc2) + b_fc2 #1024 * 17\n",
    "    \n",
    "    probabilities = tf.nn.sigmoid(fc2)\n",
    "    \n",
    "    tf.summary.histogram('W_fc2', W_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('CROSS_ENTROPY'):\n",
    "    cross_entropy =  tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits= fc2, name='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.add_to_collection(name='train_step', value=train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_beta_score = tf.Variable(0, name='f_beta_score', dtype=tf.float32)\n",
    "test_accuracy =  tf.Variable(0, name='test_accuracy',dtype=tf.float32)\n",
    "loss = tf.Variable(0, name='loss', dtype=tf.float32)\n",
    "\n",
    "tf.summary.scalar('f_beta_score', f_beta_score)\n",
    "tf.summary.scalar('test_accuracy', test_accuracy)\n",
    "tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_EPOCHS = 1\n",
    "THRESHOLD = 0.2\n",
    "DROPOuT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "    \n",
    "merged_summary = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(os.path.join('./logs',folder), graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start_time = dt.datetime.now()\n",
    "print('Starting training....')\n",
    "\n",
    "for n in range(NB_EPOCHS):\n",
    "    \n",
    "    epoch_start_time = dt.datetime.now()\n",
    "    avg_accuracy = 0\n",
    "    \n",
    "    for step,batch in enumerate(batches):\n",
    "        batch_start_time = dt.datetime.now()\n",
    "        \n",
    "        images_names = df_train.loc[batch,:].image_name_w_ext\n",
    "        images_names = images_names.apply(lambda x : os.path.join(TRAIN_PATH, x))        \n",
    "        \n",
    "        images_list = []\n",
    "        \n",
    "        for im in images_names:\n",
    "            im = cv2.imread(im, cv2.IMREAD_COLOR)\n",
    "            im = np.array(im, dtype=np.uint8)\n",
    "            im[:, :, 0] = cv2.equalizeHist(im[:, :, 0])\n",
    "            im[:, :, 1] = cv2.equalizeHist(im[:, :, 1])\n",
    "            im[:, :, 2] = cv2.equalizeHist(im[:, :, 2])\n",
    "            im = cv2.resize(im, dsize=(128,128),interpolation = cv2.INTER_CUBIC)\n",
    "            im = im / 255\n",
    "            images_list.append(im)\n",
    "        \n",
    "        images_array = np.asarray(images_list)        \n",
    "        model_name = 'mymodel_' + dt.datetime.now().strftime('%Y%m%d')\n",
    "        \n",
    "        train_step.run(feed_dict={\n",
    "                X: images_array, \n",
    "                y: df_train[ordered_labels].values[batch,:],\n",
    "                keep_prob: DROPOuT})\n",
    "        \n",
    "        predictions = tf.round(probabilities)\n",
    "      \n",
    "        s = sess.run(merged_summary, feed_dict={\n",
    "                X: images_array, \n",
    "                y: df_train[ordered_labels].values[batch,:],\n",
    "                keep_prob:1})\n",
    "\n",
    "        images_names = df_test.image_name_w_ext\n",
    "        images_names = images_names.apply(lambda x : os.path.join(TRAIN_PATH, x))        \n",
    "        \n",
    "        images_list= []\n",
    "        for im in images_names:\n",
    "            im = cv2.imread(im, cv2.IMREAD_COLOR)\n",
    "            im = np.array(im, dtype=np.uint8)\n",
    "            im[:, :, 0] = cv2.equalizeHist(im[:, :, 0])\n",
    "            im[:, :, 1] = cv2.equalizeHist(im[:, :, 1])\n",
    "            im[:, :, 2] = cv2.equalizeHist(im[:, :, 2])\n",
    "            im = cv2.resize(im, dsize=(128,128),interpolation = cv2.INTER_CUBIC)\n",
    "            im = im / 255\n",
    "            images_list.append(im)\n",
    "        \n",
    "        images_array = np.asarray(images_list)\n",
    "        \n",
    "        predictions_array = predictions.eval(feed_dict={\n",
    "            X: images_array,            \n",
    "            keep_prob: 1})\n",
    "        \n",
    "        predictions_array = predictions_array > THRESHOLD\n",
    "        predictions_array = np.asarray(predictions_array, dtype=np.int8)\n",
    "        \n",
    "        test_accuracy.assign(100 * np.mean(np.amin(np.equal(predictions_array, df_test[ordered_labels].values), axis = 1))).eval()         \n",
    "        f_beta_score.assign(fbeta_score(df_test[ordered_labels].values, predictions_array, 2, average='samples')).eval()\n",
    "        loss.assign(tf.reduce_mean(tf.reduce_sum(cross_entropy, axis=1),axis=0)).eval(feed_dict={\n",
    "            X: images_array,\n",
    "            y: df_test[ordered_labels].values,\n",
    "            keep_prob: 1})\n",
    "\n",
    "        avg_accuracy += test_accuracy.eval() / NB_MINI_BATCH\n",
    "        \n",
    "        train_writer.add_summary(s, step)\n",
    "        \n",
    "        if step%25 == 0:\n",
    "            print (predictions_array)\n",
    "            print (\"epoch: %d step: %d avg_accuracy: %.2f test_accuracy: %.2f fbeta_score: %.2f duration: %s nbTestImages: %d\" % (n,step,avg_accuracy,test_accuracy.eval(),f_beta_score.eval(),str(dt.datetime.now()-batch_start_time),images_names.size))\n",
    "            saver.save(sess, os.path.join('./ckpt', model_name), global_step=step)         \n",
    "        \n",
    "    saver.save(sess, os.path.join('./ckpt', 'mymodel_' + dt.datetime.now().strftime('%Y%m%d') + '_final.ckpt'))     \n",
    "    print(\"epoch: %d avg_accuracy: %.2f batch_accuracy: %.2f fbeta_score: %.2f duration: %s\" % (n, avg_accuracy,test_accuracy.eval(), f_beta_score.eval(),str(dt.datetime.now()-epoch_start_time)))\n",
    "\n",
    "    \n",
    "print('End of training: %s' % str(dt.datetime.now() - training_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test size needs not to be too large otherwise ==> OOM exception.\n",
    "Same thing for the batch size => we need to properly calibrate its dimension\n",
    "Initially we started with eta=0.001 et relu function\n",
    "After eta =0.0001 et elu function => weights were not learning Fast enough\n",
    "switch dropout to 0.1\n",
    "number of filters multiplies by 2 => nan of the wieghts in backpropagation => increase dropout from 0 to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
